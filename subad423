#!/bin/bash
#############################################
# author:       Christos Deligkaris
# date:         December 2016
#
# this script submits AutoDock jobs on the cluster using the SLURM job manager
#############################################

# check the arguments
if [ $# -lt 1 ]; then
	echo $'\nERROR: you need at least one argument (the AutoDock DPF input file)....\n'
	echo $'USAGE: subad4 inputfile.dpf SLURM-ARGUMENTS\n'
	echo $'SLURM-ARGUMENTS: --partition=X (partition/queue)'
	echo $'SLURM-ARGUMENTS: --ntasks=X'
	echo $'SLURM-ARGUMENTS: --cpus-per-task=X'
	echo $'INFO: Set ntasks to be the number of process you want to launch and cpus-per-task the number of threads per process.'
	echo $'INFO:For an MPI job with 8 processes, set a=8, b=1.'
	echo $'INFO:For an OpenMP job with 12 threads, set a=1, b=12.'
	echo $'INFO:For a hybrid 8-process, 12-threads per process MPI job with OpenMP 'inside', set a=8, b=12.'
	echo $'SLURM-ARGUMENTS: --ntasks-per-node=X (force your jobs to run at most at X at a time on a single node)'
	echo $'SLURM-ARGUMENTS: --mem-per-cpu=X (default is around 2GB per core, if your job exceeds the requested amount SLURM will kill it)'
	echo $'SLURM-ARGUMENTS: --exclusive (reserves the whole node for your job)\n'
        exit
fi

inputfile="$1"
outputfile="${1%.dpf}".dlg
stdoutputfile="stdoutput.log"
jobname=$(echo "$inputfile" | awk '{print substr($0,1,15)}')
jobname=AD423-"${jobname%.ad}"

# check if the input file is an ordinary file and readable
if [ ! -f "$inputfile" ] || [ ! -r "$inputfile" ] ; then
        echo "ERROR: cannot find or read input file \"${inputfile}\""
        exit
fi

# pass the sbatch arguments
sbatch_arg=
#intasks=0
#ntasks=1
for arg in $@; do
        if [ $arg != $0 ] && [ $arg != $1 ] ; then
                # pass the argument
                sbatch_arg="$sbatch_arg $arg"

# find how many cores, the amount of memory and which queue was requested (as submission arguments)
# if you want to know more do (TO DO: finish it)
#                # find how many taskswere requested
#                if [ $intasks -eq 1 ] ; then
#                        ntasks = $arg
#			intasks=0
#		fi
#                elif [ $ipartition -eq 1 ] ; then
#                        partition=$arg
#			ipartition=0
#                fi
#		elif [ $icpuspertask -eq 1 ] ; then
#			cpuspertask=$arg
#			icpuspertask=0
#		fi 
#		elif [ $intaskspernode -eq 1 ] ; then
#			ntaskspernode=$arg
#			intaskspernode=0
#		fi
#		elif [ $imempercpu -eq 1 ] ; then
#
#                #
#                if [ $arg = "--ntasks" ] ; then
#                        intasks=1
#                fi
#		elif [ $arg = "-partition" ] ; then
#			ipartition=1
#		fi
#		elif [ $arg = "-cpus-per-task" ] ; then
#			icpuspertask=1
#		fi
#		elif [ $arg = "-ntasks-per-node" ] ; then
#			intaskspernode=1
#		fi
#		elif [ $arg = "-mem-per-cpu" ] ; then
#			imempercpu=1
#		fi
        fi
done

# submit the SLURM job
sbatch ${sbatch_arg} << EOF
#!/bin/bash
##################################################################
# Define the job name
#SBATCH --job-name=$jobname
#
# Set a pattern for the output file.
#SBATCH --output=$stdoutputfile
#  By default both standard output and  standard  error are directed to a file of the name "slurm-%j.out", where the "%j" 
# is replaced with the job allocation number.   The filename pattern may contain one or more replacement symbols, which are 
# a percent sign "%" followed by a letter (e.g. %j).
#
# Supported replacement symbols are:
#     %j     Job allocation number.
#     %N     Main node name.  
#
##################################################################
# Requested number of cores. Choose either of, or both of
#
##SBATCH --ntasks=a
##SBATCH --cpus-per-task=b
#
# Set a to be the number of process you want to launch and b the number of threads per process. Typically, for an MPI job with 8 
# processes, set a=8, b=1. For an OpenMP job with 12 threads, set a=1, b=12. For a hybrid 8-process, 12-threads per process MPI 
# job with OpenMP 'inside', set a=8, b=12.
# 
# You can also set 
##SBATCH --ntasks-per-node=2
##SBATCH --exclusive
# to force your jobs to run at most at c at a time on a single node. The --exclusive option reservers the whole node for your
# job. Remove one '#' before them to activate. 
#
##################################################################
# Requested memory for each core
#
##SBATCH --mem-per-cpu=128 
#
# Set the memory requirements for the job in MB. Your job will be allocated exclusive access to that amount of RAM. In the case it
# overuses that amount, Slurm will kill it. The default value is around 2GB per core.
#
##################################################################
#
#  all environment variables will be exported to the context of the job  
#SBATCH --export=ALL
#
##################################################################

#cd \$SLURM_SUBMIT_DIR #I am not sure if I need this or not

######################

### Simple sequential job
# If you have a simple non-parallel job, just launch it. 

date
/home/christos/PROGRAMS/AUTODOCK/AUTODOCK-423/src/bin/autodock4 -p $inputfile -l $outputfile
date

######################

EOF

# end of job
