#!/bin/bash
#############################################
# author:       Christos Deligkaris
# date:         April 2018
#
# this script submits Jupyter notebook jobs on the cluster using the SLURM job manager
#
#############################################

# check the arguments
#if [ $# -lt 1 ]; then
#	echo $'\nERROR: you need at least one argument (the NWChem input file)....\n'
#	echo $'USAGE: subnwc inputfile.nw SLURM-ARGUMENTS\n'
#	echo $'SLURM-ARGUMENTS: --partition=X (partition/queue)'
#	echo $'SLURM-ARGUMENTS: --ntasks=X'
#	echo $'SLURM-ARGUMENTS: --cpus-per-task=X (cpus refers to cores)'
#	echo $'INFO: Set ntasks to be the number of process you want to launch and cpus-per-task the number of threads per process.'
#	echo $'INFO:For an MPI job with 8 processes, set a=8, b=1.'
#	echo $'INFO:For an OpenMP job with 12 threads, set a=1, b=12.'
#	echo $'INFO:For a hybrid 8-process, 12-threads per process MPI job with OpenMP 'inside', set a=8, b=12.'
#	echo $'SLURM-ARGUMENTS: --ntasks-per-node=X (force your jobs to run at most at X at a time on a single node)'
#	echo $'SLURM-ARGUMENTS: --mem-per-cpu=X (default is around 2GB per core, if your job exceeds the requested amount SLURM will kill it)'
#	echo $'SLURM-ARGUMENTS: --exclusive (reserves the whole node for your job)'
#	echo $'AVOID MEMORY ERRORS: subnwc X.nw --ntasks=1 --nodes=1 --cpus-per-task=8 --partition=medium\n'
#        exit
#fi

#inputfile="$1"
outputfile="jupyter-notebook.log"
#jobname=$(echo "$inputfile" | awk '{print substr($0,1,15)}')
jobname="jupyter-notebook"

# check if the input file is an ordinary file and readable
#if [ ! -f "$inputfile" ] || [ ! -r "$inputfile" ] ; then
#        echo "ERROR: cannot find or read input file \"${inputfile}\""
#        exit
#fi

# pass the sbatch arguments
sbatch_arg=
for arg in $@; do
 #       if [ $arg != $0 ] && [ $arg != $1 ] ; then
                # pass the argument
                sbatch_arg="$sbatch_arg $arg"
  #      fi
done

# submit the SLURM job
sbatch ${sbatch_arg} << EOF
#!/bin/bash
##################################################################
# Define the job name
#SBATCH --job-name=$jobname
#
# Set a pattern for the output file.
#SBATCH --output=$outputfile
#  By default both standard output and  standard  error are directed to a file of the name "slurm-%j.out", where the "%j" 
# is replaced with the job allocation number.   The filename pattern may contain one or more replacement symbols, which are 
# a percent sign "%" followed by a letter (e.g. %j).
#
# Supported replacement symbols are:
#     %j     Job allocation number.
#     %N     Main node name.  
#
##################################################################
#
#  all environment variables will be exported to the context of the job  
#SBATCH --export=ALL
#
##################################################################

#cd \$SLURM_SUBMIT_DIR #I am not sure if I need this or not

######################

## get tunneling info
XDG_RUNTIME_DIR=""
ipnport=\$(shuf -i8000-9999 -n1)
ipnip=\$(hostname -i)

## print tunneling instructions to jupyter-log-{jobid}.txt
echo -e "
    Copy/Paste this in your local terminal to ssh tunnel with remote
    -----------------------------------------------------------------
    ssh -N -L \$ipnport:\$ipnip:\$ipnport user@host
    -----------------------------------------------------------------

    Then open a browser on your local machine to the following address
    ------------------------------------------------------------------
    localhost:\$ipnport  (prefix w/ https:// if using password)
    ------------------------------------------------------------------
    "

## start an ipcluster instance and launch jupyter server
jupyter-notebook --no-browser --port=\$ipnport --ip=\$ipnip

######################

EOF

# end of job
